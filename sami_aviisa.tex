\documentclass[10pt,b5paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{breqn}
\usepackage[parfill]{parskip}
\usepackage{tikz}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{expex}


\usepackage[left=1.65cm,right=1.65cm,top=2cm,bottom=2cm]{geometry}
%\chead{\textsc{\small }}
%\rhead{}
%\lfoot{}
%\cfoot{\thepage}
%\rfoot{}
%\pagestyle{fancy}

\begin{document}

\title{Speech recognizer for North Sami} \author{Juho Leinonen \and Peter Smit\footnote{Contact this person, not first author} \and Mikko Kurimo} \maketitle

\begin{abstract} Speech recognizer for Sami built. N-gram tests. OOV-issues. Morphs better. \end{abstract}

\section{Introduction}

Speech recognition \cite{huang2001spoken}. North Sami. Under-resourced languages. (DigiSami \& WikiTalk)

North Sami is the largest of the nine Sami languages with about 20 000 speakers. It is a morphologically rich language like other languages in the Uralic group, as such it suffers from the same issues Finnish speech recognition suffers, inflection and cases adding many variations of the word to the lexicon. North Sami is also an under-resourced language, so it is not feasible to collect enough data to have a corpus that contains natural occurrences of the different forms the word can have. Another issue is the lack of expert help, for instance there is not enough resources to build a proper pronunciation dictionary for Sami, and therefore here, and in other model training faces, this paper introduces useful generalizations and unsupervised learning as techniques that could be applied to other under-resourced languages as well.

Words are the most common unit for n-gram language models, and they work very well for analytic and isolating languages such as English. But as already explained, they might not be the best choice for synthetic languages such as Finnish, Estonian or Sami. In these type of languages information is mostly added to the stem of the word, instead of prepositions etc. Because of this translating one Finnish word might result in many English ones, for example TELL AN EXAMPLE. For this reason, using segmented word fragments called morphs was considered as a language model unit for the recognizer. Morphs get their name from morpheme, which is the smallest grammatical unit of a language. In this paper both unsupervised and supervised learning for the optimal segmentation was tested, which resulted in morphs that represented grammatical segmentation, and ones that were generated to purely optimize the cost function of the segmentation tool, Morfessor Baseline 2.0.

Both these new language model units and words are compared to see which produces the best results in terms of error rates, both word error rate (WER) and letter error rate (LER). To make the results somewhat comparable, the total size of the language model will be restricted. This article will go over the process of building a speaker-dependent speech recognizer for Sami, for which an optimal language model will the be built.

\section{The speech recognition system}

Acoustic modelling, language modelling (more next chapter), lexicon, the decoder.


\subsection{Acoustic modelling}

Aalto university's speech recognizer is like most speech recognizers in the sense that it uses hidden Markov models (HMM) to model triphones which have use as their emission distributions Gaussian mixture models (GMM). In Finnish, which is a phonetic language, the phonemes of the triphones are generated by just assuming that every letter represents a single individual phoneme. Based on this assumption a lexicon is built. This approach is also used for Sami since it too is a phonetic language and there are not enough resources for building a proper pronunciation dictionary. This simple grapheme-to-phoneme conversion has been successfully used in other languages too [todo cite stuff].

\subsection{Language modelling}

Maybe just a little bit of something, most in the actual section.

\subsection{Lexicon /\ pronunciation dictionary}

maybe more carefully if needed. These people might actually want to see the letter-to-IPA-AaltoASR phoneme table?

\subsection{Decoding}

The decoder used in AaltoASR is a "one-pass time-synchronous decoder using re-entrant prefic-tree and token passing principle". It is optimized for Finnish but otherwise language independent. Since the main language model units in Finnish speech recognition are morphs, one problem with the decoding becomes the context-dependent phonemes at the morph boundaries, and the need create a specific model for word boundary prediction. Other issue with Finnish are the double letters for which duration modelling needs to be applied. To limit the amount of transitioning modelling between context-dependent phonemes cross-word network is embedded in the search network. All of these solutions will also be used for the decoder for Sami, since morphs will also be tried for it, and it as three different vowel lengths and double consonants. A language model lookahead is also implemented to use the LM probabilities as soon as possible to prune unlikely hypothesis before applying the full computationally heavier language model. This study suggests this length, we will use this. since morphs.

\section{Language modelling}

n-gram, lexicon, words, morphs, morfessor. 

N-gram word models are the most popular method for language modelling. They will also be used here, except using morphs instead of words will also be tested. Since in Sami the amount of word types rises very quickly with increasing corpus size (maybe tests about this with the larger corpus?), limiting the vocabulary might become needed, so tests are done to see its effect on the error rates, and n-gram amounts (speed). For segmenting words into morphs both supervised and unsupervised methods are tested.

\subsection{Words}

Repetition?

\subsection{Morphs}

Morphs get their name from morphemes, but here they are treated as just word fragments. The size of a word can be anything from a single letter to a whole word. While with words collecting them to a lexicon is a simple task of just gathering every individual word, and if need be limiting by maybe amount of occurances, a special tool is used to gathering from a corpus and then segmenting to them.

\subsubsection{Morfessor}

Morfessor 

\section{Experiments} 

\section{Results}

\section{Discussion}

\section{Conclusions} 

\section{Acknowledgements} 

\bibliographystyle{unsrt}
\bibliography{mybib} 

\end{document}